{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\scaledge_projects\\\\bird_species_classification\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\scaledge_projects\\\\bird_species_classification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    testing_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_lr_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bird_classification.utils import *\n",
    "from src.bird_classification.utils.common import create_directories\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUGMENTATION': True, 'IMAGE_SIZE': [224, 224, 3], 'BATCH_SIZE': 32, 'INCLUDE_TOP': False, 'EPOCHS': 2, 'CLASSES': 10, 'MOMENTUM': 0.9, 'LEARNING_RATE': 0.01}\n",
      "{'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/Sibasis555/Face_mask_detection/raw/main/datasets/dummy_bird_species.zip', 'local_data_file': 'artifacts/data_ingestion/dummy_bird_species.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'prepare_base_model': {'root_dir': 'artifacts/prepare_base_model', 'base_model_path': 'artifacts/prepare_base_model/base_model.pth', 'updated_base_model_path': 'artifacts/prepare_base_model/base_model_updated.pth'}, 'prepare_callbacks': {'root_dir': 'artifacts/prepare_callbacks', 'tensorboard_root_log_dir': 'artifacts/prepare_callbacks/tensorboard_log_dir', 'checkpoint_model_filepath': 'artifacts/prepare_callbacks/checkpoint_dir/model.pth'}, 'training': {'train_data': 'artifacts/data_ingestion/dummy_bird_species/Train', 'test_data': 'artifacts/data_ingestion/dummy_bird_species/Test', 'root_dir': 'artifacts/training', 'trained_model_path': 'artifacts/training/model.pth'}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"src/bird_classification/utils/params.yaml\", 'r') as file:\n",
    "        params = yaml.safe_load(file)\n",
    "        print(params)\n",
    "\n",
    "with open(\"src/bird_classification/utils/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager: #Config Entity data\n",
    "    def __init__(self):\n",
    "        get_config_and_param()\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "    \n",
    "        create_directories([self.config[\"artifacts_root\"]])\n",
    "        # print(self.config)\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config[\"training\"]\n",
    "        prepare_base_model = self.config[\"prepare_base_model\"]\n",
    "        params = self.params\n",
    "        # print(self.config[\"training\"])\n",
    "        training_data = self.config[\"training\"][\"train_data\"]\n",
    "        testing_data = self.config[\"training\"][\"test_data\"]\n",
    "        create_directories([\n",
    "            Path(training[\"root_dir\"])\n",
    "        ])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training[\"root_dir\"]),\n",
    "            trained_model_path=Path(training[\"trained_model_path\"]),\n",
    "            updated_base_model_path=Path(prepare_base_model[\"updated_base_model_path\"]),\n",
    "            training_data=Path(training_data),\n",
    "            testing_data=Path(testing_data),\n",
    "            params_epochs=params[\"EPOCHS\"],\n",
    "            params_batch_size=params[\"BATCH_SIZE\"],\n",
    "            params_is_augmentation=params[\"AUGMENTATION\"],\n",
    "            params_image_size=params[\"IMAGE_SIZE\"],\n",
    "            params_lr_rate=params[\"LEARNING_RATE\"]\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(data_folder)\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(data_folder, class_name)\n",
    "            images = os.listdir(class_path)\n",
    "            self.image_paths.extend([os.path.join(class_path, img) for img in images])\n",
    "            self.labels.extend([class_idx] * len(images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            # print('before',image.size)\n",
    "            image = self.transform(image)\n",
    "            # print('after',image.size)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessing:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.train_path = self.config.training_data\n",
    "        self.test_path = self.config.testing_data\n",
    "        # print(\"path--\",self.config.training_data)\n",
    "        \n",
    "    def data_normalization(self):\n",
    "        data_transforms = transforms.Compose([\n",
    "        transforms.Resize(size=(224,224), interpolation=Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "        ])\n",
    "        #Getting the data\n",
    "        # print(self.train_path)\n",
    "        self.train_set = ImageDataset(self.config.training_data, data_transforms)\n",
    "        self.test_set = ImageDataset(self.config.testing_data, data_transforms)\n",
    "\n",
    "        self.class_name=self.train_set.classes\n",
    "\n",
    "        return self.train_set, self.test_set\n",
    "    \n",
    "    def data_lode(self):\n",
    "        train_loader = DataLoader(self.train_set, batch_size=8, shuffle=True)\n",
    "        test_loader = DataLoader(self.test_set, batch_size=8, shuffle=False)\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, train_loader, test_loader, train_set, test_set, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        # print(self.config)\n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr= self.config.params_lr_rate)\n",
    "\n",
    "    @staticmethod \n",
    "    def get_num_correct(preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "    def model_training(self):\n",
    "        tb = SummaryWriter()\n",
    "        device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = self.model.to(device)\n",
    "        start_time=time.time()\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            epoch_time=time.time()\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            valid_correct=0\n",
    "            valid_loss=0\n",
    "            model.train()\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images)\n",
    "\n",
    "                loss = self.criterion(preds, labels)\n",
    "                total_loss+= loss.item()\n",
    "                total_correct+= self.get_num_correct(preds, labels)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "            tb.add_scalar(\"Correct\", total_correct, epoch)\n",
    "            tb.add_scalar(\"Accuracy\", (total_correct/ len(self.train_set))*100, epoch)\n",
    "            for name, weight in model.named_parameters():\n",
    "                tb.add_histogram(name, weight, epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight, epoch)\n",
    "            print(\"Train Accuracy...\")\n",
    "            print(\"epoch:\", epoch, f\"total_correct: [{total_correct}/461]\", \"loss:\",total_loss, \"Accuracy\", (total_correct/ len(self.train_set))*100)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for images, labels in self.test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    preds = model(images)\n",
    "\n",
    "                    loss = self.criterion(preds, labels)\n",
    "                    valid_loss+= loss.item()\n",
    "                    valid_correct+= self.get_num_correct(preds, labels)\n",
    "            total_epoch_time=time.time() - epoch_time\n",
    "            print(\"Test Accuracy...\")\n",
    "            print(f\"total_correct: [{valid_correct}/116]\", \"loss:\",valid_loss, \"Accuracy\", (valid_correct/ len(self.test_set))*100)\n",
    "            print(f\"The model with [{epoch}/{self.config.params_epochs}+1] epoch took {total_epoch_time/60} minutes\")\n",
    "        tb.close()\n",
    "        total_time=time.time() - start_time\n",
    "        print(f\"The model with {self.config.params_epochs} epochs took {total_time/60} minutes\")\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "    @staticmethod\n",
    "    def save_model(path, model):\n",
    "        print(\"saving model ...\")\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\bird_classification\\utils\\config.yaml\n",
      "[2023-07-24 09:39:42,351: INFO: common: created directory at: artifacts]\n",
      "[2023-07-24 09:39:42,352: INFO: common: created directory at: artifacts\\training]\n",
      "epoch: 0 total_correct: [38/461] loss: 137.84589982032776 Accuracy 8.24295010845987\n",
      "total_correct: [10/116] loss: 35.66346716880798 Accuracy 8.620689655172415\n",
      "The model with [0/2] epoch took 676.8347330093384 seconds\n",
      "epoch: 1 total_correct: [47/461] loss: 136.88063836097717 Accuracy 10.195227765726681\n",
      "total_correct: [12/116] loss: 35.41725254058838 Accuracy 10.344827586206897\n",
      "The model with [1/2] epoch took 634.8944907188416 seconds\n",
      "The model with 2 epochs took 1311.7365295886993 seconds\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    # prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    # prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    # callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
    "\n",
    "    training_config = config.get_training_config()\n",
    "\n",
    "    data_preprocessing = Data_preprocessing(config=training_config)\n",
    "    train_set, test_set = data_preprocessing.data_normalization()\n",
    "    train_loader, test_loader = data_preprocessing.data_lode()\n",
    "\n",
    "    training = Training(train_loader, test_loader, train_set, test_set, config=training_config)\n",
    "    training.get_base_model()\n",
    "    # training.train_valid_generator()\n",
    "    training.model_training()    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
